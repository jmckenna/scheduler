{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dagster","text":""},{"location":"#about","title":"About","text":"<p>The following is a description of the steps and requirements for building and deploying the docker based workflow implemented in  dagster.</p>"},{"location":"#overview","title":"Overview","text":"<p>The image following provides a broad overview of the elements that  are loaded in to the Docker orchestration environment.  This is a very  basic view and doesn't present any scaling or fail over elements.  </p> <p>The key elements are:</p> <ul> <li>sources to configuration and then the creation of the archive files that are loaded and used to load into the Gleaner and Nabu tools</li> <li>The Dagster set which loads three containers to support workflow operations</li> <li>The Gleaner Architecture images which loads three or more containers to support </li> <li>s3 object storage</li> <li>graph database (triplestore)</li> <li>headless chrome for page rendering to support dynamically inserted JSON-LD</li> <li>any other support packages like text, semantic or spatial indexes</li> <li>The GleanerIO tools which loads two containers  as services (Gleaner and Nabu) that are run  and removed by the Dagster workflow</li> </ul> <p></p>"},{"location":"#template-files","title":"Template files","text":"<p>The template files define the Dagster Ops, Jobs and Schedules.  From these and a GleanerIO config file a set of Python scripts for Dagster are created in the output directory. </p> <p>These only need to be changed or used to regenerate if you wish to alter the  execution graph (ie, the ops, jobs and schedules) or change the config file. In the later case only a regeneration needs to be done.</p> <p>There are then Docker build scripts to build out new containers.  </p> <p>See:  template</p>"},{"location":"#steps-to-build-and-deploy","title":"Steps to build and deploy","text":"<p>The deployment can be tested locally using docker. The production 'containers' are built with a github action, or using a makefile.</p> <p>This describes the local and container deployment We use portainer to manage our docker deployments.</p> <p>1) move to the the deployment directory 2) copy the envFile.env to .env  3) edit the entries. 4) for local, <code>./dagster_localrun.sh</code> 5) go to http://localhost:3000/</p> <p>To deploy in portainer, use the deployment/compose_project.yaml docker stack.</p>"},{"location":"#docker-compose-configuration","title":"docker compose Configuration:","text":"<p>1) there are three files that need to be installed into docker configs. </p> file local stack note workspace configs/PROJECT/worksapce.yaml env () used by dagster gleanerconfig.yaml configs/PROJECT/gleanerconfigs.yaml env () needs to be in portainer nabuconfig.yaml configs/PROJECT/nabuconfigs.yaml env () needs to be in portainer 2)"},{"location":"#editing-template","title":"Editing Template","text":"<p>you can edit implnets/template</p> <p>then deploy with</p> <p>`pygen.py -cf ./configs/eco/gleanerconfig.yaml -od ./generatedCode/implnet-eco/output -td ./templates/v1 -d 7 ``</p> <p>If you are running using dagster_localrun.sh  1) go to the deployment at http://localhost:3000/locations 2) click 'reload on gleaner@project_grpc' 3) then if code is correct, then you will be able run the changed workflows</p> <p>(TODO NEEDS MORE )</p>"},{"location":"#makefile","title":"MAKEFILE","text":"<p>1) Place your gleanerconfig.yaml (use that exact name) in confgis/NETWORK/gleanerconfig.yaml    1) Note:  When doing your docker build, you will use this NETWORK name as a value in the command such as    <pre><code>podman build  --tag=\"docker.io/fils/dagster_nsdf:$(VERSION)\"  --build-arg implnet=nsdf --file=./build/Dockerfile </code></pre> 1) Make any needed edits to the templates in directory templates/v1/ or make your own template set in that directory</p> <p>The command to build using the pygen.py program follows.  This is done from the standpoint of running in from the  implenet directory.</p> <pre><code> python pygen.py -cf ./configs/nsdf/gleanerconfig.yaml -od ./generatedCode/implnet-nsdf/output  -td ./templates/v1   -d 7</code></pre> <p>1) This will generate the code to build a dagster instance from the combination of the templates and gelanerconfig.yaml. 2) </p>"},{"location":"#environment-files","title":"Environment files","text":"<p>1) cp deployment/envFile.env .env 2) edit 3) <code>export $(cat .env | xargs)</code> export $(cat .env | xargs) <pre><code>######\n# Nabu and Gleaner configs need to be in docker configs\n## docker config name GLEANER_GLEANER_DOCKER_CONFIG\n## docker config name GLEANER_NABU_DOCKER_CONFIG\n#        suggested DOCKER_CONFIG NAMING PATTERN (nabu||gleaner)-{PROJECT}\n########\nGLEANERIO_GLEANER_DOCKER_CONFIG=gleaner-eco\nGLEANERIO_NABU_DOCKER_CONFIG=nabu-eco\n\n# ###\n# workspace for dagster\n####\nGLEANERIO_WORKSPACE_CONFIG_PATH=/usr/src/app/workspace.yaml\nGLEANERIO_WORKSPACE_DOCKER_CONFIG=workspace-eco\n\n\n# NETWORK is needed for headless rendering\n# gleaner\n\n\nDEBUG=False\nPROJECT=eco\n#PROJECT=iow\n#PROJECT=oih\nHOST=localhost\nPROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n# port is required: https://portainer.{HOST}:443/api/endpoints/2/docker/\nPORTAINER_URL=\nPORTAINER_KEY=\n\n# Network\nGLEANERIO_HEADLESS_NETWORK=headless_gleanerio\n\n### GLEANER/NABU Dockers\nGLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:latest\nGLEANERIO_NABU_IMAGE=nsfearthcube/nabu:latest\n\n\n\n##\n# path where configs are deployed/mounted\n####\nGLEANERIO_GLEANER_CONFIG_PATH=/gleaner/gleanerconfig.yaml\nGLEANERIO_NABU_CONFIG_PATH=/nabu/nabuconfig.yaml\n###\n\n\n\n\n\n#GLEANERIO_LOG_PREFIX=scheduler/logs/\n\nGLEANERIO_MINIO_ADDRESS=\nGLEANERIO_MINIO_PORT=80\nGLEANERIO_MINIO_USE_SSL=false\nGLEANERIO_MINIO_BUCKET=\nGLEANERIO_MINIO_ACCESS_KEY=\nGLEANERIO_MINIO_SECRET_KEY=\nGLEANERIO_HEADLESS_ENDPOINT=http://headless:9222\n\n\n\n# just the base address, no namespace https://graph.geocodes-aws-dev.earthcube.org/blazegraph\nGLEANERIO_GRAPH_URL=\nGLEANERIO_GRAPH_NAMESPACE=\n\n</code></pre></p>"},{"location":"#implementation-networks","title":"Implementation Networks","text":"<p>This (https://github.com/sharmasagar25/dagster-docker-example)  is an example on how to structure a [Dagster] project in order to organize the jobs, repositories, schedules, and ops. The example also contains examples on unit-tests and a docker-compose deployment file that utilizes a Postgresql database for the run, event_log and schedule storage.</p> <p>This example should in no way be considered suitable for production and is merely my own example of a possible file structure. I personally felt that it was difficult to put the Dagster concepts to use since the projects own examples had widely different structure and was difficult to overview as a beginner.</p> <p>The example is based on the official [tutorial].</p>"},{"location":"#folders","title":"Folders","text":"<ul> <li>build:  build directives for the docker containers</li> <li>configs</li> <li>src</li> <li>tooling</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<p>At this point it is expected that you have a valid Gleaner config file named gleanerconfig.yaml located in some path within the configs directory.</p>"},{"location":"#building-the-dagster-code-from-templates","title":"Building the dagster code from templates","text":"<p>The python program pygen will read a gleaner configuration file and a set of  template and build the Dagster code from there.  </p> <pre><code>python pygen.py -cf ./configs/nsdf/gleanerconfig.yaml -od ./src/implnet-nsdf/output  -td ./src/implnet-nsdf/templates  -d 7</code></pre>"},{"location":"#running","title":"Running","text":"<p>There is an example on how to run a single pipeline in <code>src/main.py</code>. First install the dependencies in an isolated Python environment.</p> <pre><code>pip install -r requirements</code></pre> <p>The code built above can be run locally, though your templates may be set up  to reference services and other resources not present on your dev machine.  For  complex examples like these, it can be problematic.  </p> <p>If you are looking for some simple examples of Dagster, check out the directory examples for some smaller self-contained workflows.  There are good for testing things like sensors and other approaches. </p> <p>If you wish to still try the generated code cd into the output directory you specified in the pygen command.</p> <p>Then use:</p> <pre><code>dagit -h ghost.lan -w workspace.yaml</code></pre>"},{"location":"#building","title":"Building","text":"<pre><code> podman build  -t  docker.io/fils/dagster:0.0.24  .</code></pre> <pre><code> podman push docker.io/fils/dagster:0.0.24</code></pre>"},{"location":"#appendix","title":"Appendix","text":""},{"location":"#setup","title":"Setup","text":""},{"location":"#docker-api-sequence","title":"Docker API sequence","text":""},{"location":"#appendix_1","title":"Appendix","text":""},{"location":"#portainer-api-setup","title":"Portainer API setup","text":"<p>You will need to setup Portainer to allow for an API call.  To do this look  at the documentation for Accessing the Portainer API</p>"},{"location":"#notes","title":"Notes","text":"<p>Single file testing run</p> <pre><code> dagit -h ghost.lan -f test1.py</code></pre> <ul> <li>Don't forget to set the DAGSTER_HOME dir like in </li> </ul> <pre><code> export DAGSTER_HOME=/home/fils/src/Projects/gleaner.io/scheduler/python/dagster</code></pre> <pre><code>dagster-daemon run</code></pre> <p>Run from directory where workspace.yaml is. <pre><code>dagit --host 192.168.202.159</code></pre></p>"},{"location":"#cron-notes","title":"Cron Notes","text":"<p>A useful on-line tool:  https://crontab.cronhub.io/</p> <pre><code>0 3 * * *   is at 3 AM each day\n\n0 3,5 * * * at 3 and 5 am each day\n\n0 3 * * 0  at 3 am on Sunday\n\n0 3 5 * *  At 03:00 AM, on day 5 of the month\n\n0 3 5,19 * * At 03:00 AM, on day 5 and 19 of the month\n\n0 3 1/4 * * At 03:00 AM, every 4 days</code></pre>"},{"location":"#indexing-approaches","title":"Indexing Approaches","text":"<p>The following approaches</p> <ul> <li>Divide up the sources by sitemap and sitegraph</li> <li>Also divide by production and queue sources</li> </ul> <p>The above will result in at most 4 initial sets.</p> <p>We can then use the docker approach</p> <pre><code>./gleanerDocker.sh -cfg /gleaner/wd/rundir/oih_queue.yaml  --source cioosatlantic</code></pre> <p>to run indexes on specific sources in these configuration files.  </p>"},{"location":"#references","title":"References","text":"<ul> <li>Simple Dagster example</li> </ul>"},{"location":"README_LOCAL_DEVELOPMENT/","title":"Development","text":"<p>If you look in the doc/README.md that description is probably better.</p> <p>Two types:</p> <p>2) dagster dev   - Dagster runs the UI in development mode 1) Container based. This uses docker and locally deployed containers</p> <p>!!!  note      NOTE, the Dagster and the Code containers need to be the same.     For local development images are named <code>dagster-local:latest</code> and code containers named dagster-gleanerio-local:latest     and built in the compose_local.yaml     for production,       * dagster named: nsfearthcube/dagster-gleanerio:${CONTAINER_DAGSTER_TAG:-latest}       * code containers  are named <code>nsfearthcube/dagster-gleanerio-${PROJECT:-eco}:${CONTAINER_TAG:-latest}</code>     eg in dockerhub.com as nsfearthcube/dagster-eco:latest</p>"},{"location":"README_LOCAL_DEVELOPMENT/#dagster-dev","title":"DAGSTER DEV","text":"<p>At the top level (dagster/implents) you can run </p> <p><code>dagster dev</code></p> <p>You need to set the environment based on dagster/implnets/deployment/envFile.env</p> <p>It should run workflows/tasks/tasks</p> <p>defined in the pyproject.toml</p> <pre><code>[tool.dagster]\nmodule_name = \"workflows.tasks.tasks\"</code></pre>"},{"location":"README_LOCAL_DEVELOPMENT/#setting-up-in-pycharm","title":"Setting up in pycharm","text":"<p>you can add runconfigs in pycharm </p> <p>You should/need to add the envFile plug in so that env files can be  </p>"},{"location":"README_LOCAL_DEVELOPMENT/#testing-tasks","title":"testing tasks","text":"<p><code>cd dagster/implnets/workflows/tasks</code> You need to set the environment based on dagster/implnets/deployment/envFile.env</p> <p><code>export $(sed  '/^[ \\t]*#/d' ../../deployment/.env |  sed '/^$/d' | xargs)</code></p> <p><code>dagster dev</code></p> <p>will run just the task, and in editable form, i think.</p>"},{"location":"README_LOCAL_DEVELOPMENT/#testing-generated-code","title":"testing generated code","text":"<p><code>cd generatedCode/PROJECT/output/</code></p> <p><code>export $(sed  '/^[ \\t]*#/d' ../../../deployment/.env |  sed '/^$/d' | xargs)</code></p> <p><code>dagster dev</code></p> Note <p>YOU CANNOT SET BREAKPOINTS IN TEMPLATES YOU NEED TO cd generatedCode/PROJECT/output/jobs and set them in the job you are testing.</p>"},{"location":"README_LOCAL_DEVELOPMENT/#testing-containers","title":"TESTING CONTAINERS","text":"<p>Containers are a well tested approach. We deploy these container to production, so it's a good way to test. There are a set of required files:</p> <ul> <li>env variables file</li> <li>gleaner/nabu configuration files, without any passwords, servers. Those are handled in the env variables</li> <li>docker compose file</li> <li>docker networks and volumes for the compose files</li> <li>three files uploaded to docker as configs<ul> <li>gleanerconfigs.yaml gleaner/nabu</li> <li>nabuconfigs.yaml - gleaner/nabu</li> <li>workspace.yaml -- dagster</li> </ul> </li> <li>(opptional/advanced) add a compose_project_PROJECT_override.yaml file with additional containers</li> </ul>"},{"location":"README_LOCAL_DEVELOPMENT/#portainer-api-key","title":"PORTAINER API KEY","text":"<p>note on how to do this. ''</p>"},{"location":"README_LOCAL_DEVELOPMENT/#start","title":"Start","text":"<p>For production environments, script, <code>dagster_setup_docker.sh</code>  should create the networks, volumes, and  upload configuration files</p> <p>1) setup a project in configs directory, if one des not exist     2)   add gleanerconfig.yaml, nabuconfig.yaml, and workspace.yaml (NOTE NEED A TEMPLATE FOR THIS) 1) copy envFile.env to .env, and edit 2) run  ./dagster_localrun.sh 4) go to https://loclahost:3000/ 5) run a small test dataset.</p> <pre><code>cd dagster/implnets/deployment\ncp envFile.env .env\n# configure environment in .env \n\n./dagster_localrun.sh\n</code></pre> <p>If you look in dagster_localrun.sh you can see that the  $PROJECT variable is used to define what files to use, and define, and to setup a separate 'namespace' in traefik labels.</p> <p>If you look in compose_local_eco_override.yaml you can see that additional mounts are added to the containers.</p> <p>These can be customized in the  <code>compose_local_PROJECT_override.yaml</code> for local development.</p>"},{"location":"README_LOCAL_DEVELOPMENT/#customizing-the-configs","title":"customizing the configs","text":"<p>for local development three configs</p> <ul> <li>configs/PROJECT/gleanerconfigs.yaml gleaner/nabu</li> <li>configs/PROJECT/nabuconfigs.yaml - gleaner/nabu</li> <li>configs/PROJECT/workspace.yaml -- dagster</li> </ul>"},{"location":"README_LOCAL_DEVELOPMENT/#editingtesting-code","title":"Editing/testing code","text":"<p>if you run pygen, then you need to regnerate code. the makefile or a pycharm run config is the best way. </p>"},{"location":"README_LOCAL_DEVELOPMENT/#moving-to-production","title":"MOVING TO PRODUCTION","text":"<p>(NOTE NEED SOME MAKEFILES FOR THIS.)</p> <p>you need to create a compose_project_PROJECT_override.yaml</p> <p>After copying fragment from <code>compose_local_PROJECT_override.yaml</code> 1) CHANGE THE IMAGE TO <code>docker.io/nsfearthcube/dagster-gleanerio-${PROJECT:-eco}:${CONTAINER_CODE_TAG:-latest}</code> 2) remove the line: platform: linux/x86_64</p>"},{"location":"README_LOCAL_DEVELOPMENT/#for-portainer","title":"For portainer,","text":"<p>Create a stack, and add override file using  \"additional_file\" to add this to the stack</p>"},{"location":"README_LOCAL_DEVELOPMENT/#command-line-deploy","title":"command line deploy","text":"<p>docker compose -env .env -f compose_project.yaml -f compose_project_PROJECT_override.yaml up</p>"},{"location":"README_LOCAL_DEVELOPMENT/#system-not-supporting-multiple-configs","title":"system not supporting multiple configs","text":"<p>If you are not using portainer, you need to create a merged config file.</p> <p>Then you will merge the files. Preview with: </p> <p><code>docker compose -f compose_project.yaml -f compose_project_PROJECT_override.yaml config</code></p> <p>this should show you  a merged file.</p> <p><code>docker compose -f compose_project.yaml -f compose_project_PROJECT_override.yaml config  &gt; compose_project_PROJECT.yaml</code></p> <p>then start docker compose with the merged file.</p>"},{"location":"add_containers/","title":"Containers for Dagster Scheduler","text":""},{"location":"add_containers/#things-to-work-on","title":"Things to work on","text":"<ul> <li>secrets</li> <li>modify compose to use a project variable </li> <li>network modify to use passed evn for network names</li> </ul> <p>Future: * can we use a volume. Use git to pull?</p>"},{"location":"add_containers/#build-docker","title":"Build Docker","text":"<p>Needs to be automated with a workflow</p> <p>something about archive file</p>"},{"location":"add_containers/#add-stack","title":"add Stack.","text":"<p>dagster/deployment/compose.yaml make usre that it is the correct version.   image: docker.io/fils/dagster_eco:0.0.44</p> <p>NOTE: we can use a env ${project} varibale like we do for geocodes_.._named.yaml</p>"},{"location":"alternatives/","title":"Alternative Approaches","text":""},{"location":"alternatives/#abouts","title":"Abouts","text":"<p>These are just some scratch notes on some of the approaches for  job workflows.  This is a more historical document than anything. </p>"},{"location":"alternatives/#options","title":"Options","text":"<ul> <li>Dagster<ul> <li>Dagster</li> <li>Build ETL pipelines</li> <li>Create new project</li> </ul> </li> <li>Red Engine<ul> <li>Red Engine</li> <li>HN Posting on Red Engine</li> <li>Red Engine read the docs</li> <li>Rocketry</li> </ul> </li> <li>Dramtiq<ul> <li>Dramatic</li> </ul> </li> <li>Temporalio<ul> <li>Temporalio</li> </ul> </li> <li>Luigi<ul> <li>Luigi</li> </ul> </li> <li>AppScheduler<ul> <li>AppScheduler</li> </ul> </li> <li>Go Cadance</li> <li>Airflow</li> <li>Background jobs in linux</li> <li>exec-command</li> </ul>"},{"location":"alternatives/#other-dagster-references","title":"Other Dagster References","text":"<ul> <li>pybokeh/dagster-sklearn</li> <li>Gave me the inspiration for the primary folder structure. Although that     example is more advanced and utilizes sklearn.</li> <li>dagster-io/dagster examples</li> <li>Dagster's own examples.</li> <li>xyzy-web/dagster-exchangerates</li> <li>An example that includes Kubernetes Deployment.</li> <li>sephib/dagster-graph-project</li> <li>sspaeti-com/practical-data-engineering</li> </ul>"},{"location":"developement/","title":"Scheduler Developement in Dagster","text":"<p>Note</p> <p>add envfile plug in to PyCharm to allow for easy debugging to code</p>"},{"location":"developement/#about","title":"About","text":"<p>The following is a description of the steps and requirements for building and deploying the docker based workflow implemented in  dagster.</p> <p>when developing, you can use <code>dagster dev</code> you need to have the environment variables defined, so it's easist to do in a Pycharm run shell script.</p> <p>You can use a python module, dagster  with option dev load the runConfiguration dagster_eco_debug.run.xml</p>"},{"location":"developement/#template-files","title":"Template files","text":"<p>The template files define the Dagster Ops, Jobs and Schedules.  From these and a GleanerIO config file a set of Python scripts for Dagster are created in the output directory. </p> <p>These only need to be changed or used to regenerate if you wish to alter the  execution graph (ie, the ops, jobs and schedules) or change the config file. In the later case only a regeneration needs to be done.</p> <p>There are then Docker build scripts to build out new containers.  </p> <p>See:  template</p>"},{"location":"developement/#steps-to-build-and-deploy","title":"Steps to build and deploy","text":"<p>The deployment can be tested locally using docker. The production 'containers' are built with a github action, or using a makefile.</p> <p>This describes the local and container deployment We use portainer to manage our docker deployments.</p> <p>1) move to the the deployment directory 2) copy the envFile.env to .env  3) edit the entries. 4) for local, <code>./dagster_localrun.sh</code> 5) go to http://localhost:3000/</p> <p>To deploy in portainer, use the deployment/compose_project.yaml docker stack.</p>"},{"location":"developement/#docker-compose-configuration","title":"docker compose Configuration:","text":"<p>1) there are three files that need to be installed into docker configs. </p> file local stack note workspace configs/PROJECT/worksapce.yaml env () used by dagster gleanerconfig.yaml configs/PROJECT/gleanerconfigs.yaml env () needs to be in portainer nabuconfig.yaml configs/PROJECT/nabuconfigs.yaml env () needs to be in portainer 2)"},{"location":"developement/#editing-template","title":"Editing Template","text":"<p>you can edit implnets/template</p> <p>then deploy with</p> <p>`pygen.py -cf ./configs/eco/gleanerconfig.yaml -od ./generatedCode/implnet-eco/output -td ./templates/v1 -d 7 ``</p> <p>If you are running using dagster_localrun.sh  1) go to the deployment at http://localhost:3000/locations 2) click 'reload on gleaner@project_grpc' 3) then if code is correct, then you will be able run the changed workflows</p> <p>(TODO NEEDS MORE )</p>"},{"location":"developement/#environment-files","title":"Environment files","text":"<p>1) cp deployment/envFile.env .env 2) edit 3) <code>export $(cat .env | xargs)</code> export $(cat .env | xargs) <pre><code>######\n# Nabu and Gleaner configs need to be in docker configs\n## docker config name GLEANER_GLEANER_DOCKER_CONFIG\n## docker config name GLEANER_NABU_DOCKER_CONFIG\n#        suggested DOCKER_CONFIG NAMING PATTERN (nabu||gleaner)-{PROJECT}\n########\nGLEANERIO_GLEANER_DOCKER_CONFIG=gleaner-eco\nGLEANERIO_NABU_DOCKER_CONFIG=nabu-eco\n\n# ###\n# workspace for dagster\n####\nGLEANERIO_WORKSPACE_CONFIG_PATH=/usr/src/app/workspace.yaml\nGLEANERIO_WORKSPACE_DOCKER_CONFIG=workspace-eco\n\n\n# NETWORK is needed for headless rendering\n# gleaner\n\n\nDEBUG=False\nPROJECT=eco\n#PROJECT=iow\n#PROJECT=oih\nHOST=localhost\nPROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n# port is required: https://portainer.{HOST}:443/api/endpoints/2/docker/\nPORTAINER_URL=\nPORTAINER_KEY=\n\n# Network\nGLEANERIO_HEADLESS_NETWORK=headless_gleanerio\n\n### GLEANER/NABU Dockers\nGLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:latest\nGLEANERIO_NABU_IMAGE=nsfearthcube/nabu:latest\n\n\n\n##\n# path where configs are deployed/mounted\n####\nGLEANERIO_GLEANER_CONFIG_PATH=/gleaner/gleanerconfig.yaml\nGLEANERIO_NABU_CONFIG_PATH=/nabu/nabuconfig.yaml\n###\n\n\n\n\n\n#GLEANERIO_LOG_PREFIX=scheduler/logs/\n\nGLEANERIO_MINIO_ADDRESS=\nGLEANERIO_MINIO_PORT=80\nGLEANERIO_MINIO_USE_SSL=false\nGLEANERIO_MINIO_BUCKET=\nGLEANERIO_MINIO_ACCESS_KEY=\nGLEANERIO_MINIO_SECRET_KEY=\nGLEANERIO_HEADLESS_ENDPOINT=http://headless:9222\n\n\n\n# just the base address, no namespace https://graph.geocodes-aws-dev.earthcube.org/blazegraph\nGLEANERIO_GRAPH_URL=\nGLEANERIO_GRAPH_NAMESPACE=\n\n</code></pre></p>"},{"location":"developement/#implementation-networks","title":"Implementation Networks","text":"<p>This (https://github.com/sharmasagar25/dagster-docker-example)  is an example on how to structure a [Dagster] project in order to organize the jobs, repositories, schedules, and ops. The example also contains examples on unit-tests and a docker-compose deployment file that utilizes a Postgresql database for the run, event_log and schedule storage.</p> <p>This example should in no way be considered suitable for production and is merely my own example of a possible file structure. I personally felt that it was difficult to put the Dagster concepts to use since the projects own examples had widely different structure and was difficult to overview as a beginner.</p> <p>The example is based on the official [tutorial].</p>"},{"location":"developement/#folders","title":"Folders","text":"<ul> <li>build:  build directives for the docker containers</li> <li>configs</li> <li>src</li> <li>tooling</li> </ul>"},{"location":"developement/#requirements","title":"Requirements","text":"<p>At this point it is expected that you have a valid Gleaner config file named gleanerconfig.yaml located in some path within the configs directory.</p>"},{"location":"developement/#building-the-dagster-code-from-templates","title":"Building the dagster code from templates","text":"<p>The python program pygen will read a gleaner configuration file and a set of  template and build the Dagster code from there.  </p> <pre><code>python pygen.py -cf ./configs/nsdf/gleanerconfig.yaml -od ./src/implnet-nsdf/output  -td ./src/implnet-nsdf/templates  -d 7</code></pre>"},{"location":"developement/#running","title":"Running","text":"<p>There is an example on how to run a single pipeline in <code>src/main.py</code>. First install the dependencies in an isolated Python environment.</p> <pre><code>pip install -r requirements</code></pre> <p>The code built above can be run locally, though your templates may be set up  to reference services and other resources not present on your dev machine.  For  complex examples like these, it can be problematic.  </p> <p>If you are looking for some simple examples of Dagster, check out the directory examples for some smaller self-contained workflows.  There are good for testing things like sensors and other approaches. </p> <p>If you wish to still try the generated code cd into the output directory you specified in the pygen command.</p> <p>Then use:</p> <pre><code>dagit -h ghost.lan -w workspace.yaml</code></pre>"},{"location":"developement/#building","title":"Building","text":"<pre><code> podman build  -t  docker.io/fils/dagster:0.0.24  .</code></pre> <pre><code> podman push docker.io/fils/dagster:0.0.24</code></pre>"},{"location":"developement/#appendix","title":"Appendix","text":""},{"location":"developement/#setup","title":"Setup","text":""},{"location":"developement/#docker-api-sequence","title":"Docker API sequence","text":""},{"location":"developement/#appendix_1","title":"Appendix","text":""},{"location":"developement/#portainer-api-setup","title":"Portainer API setup","text":"<p>You will need to setup Portainer to allow for an API call.  To do this look  at the documentation for Accessing the Portainer API</p>"},{"location":"developement/#notes","title":"Notes","text":"<p>Single file testing run</p> <pre><code> dagit -h ghost.lan -f test1.py</code></pre> <ul> <li>Don't forget to set the DAGSTER_HOME dir like in </li> </ul> <pre><code> export DAGSTER_HOME=/home/fils/src/Projects/gleaner.io/scheduler/python/dagster</code></pre> <pre><code>dagster-daemon run</code></pre> <p>Run from directory where workspace.yaml is. <pre><code>dagit --host 192.168.202.159</code></pre></p>"},{"location":"developement/#cron-notes","title":"Cron Notes","text":"<p>A useful on-line tool:  https://crontab.cronhub.io/</p> <pre><code>0 3 * * *   is at 3 AM each day\n\n0 3,5 * * * at 3 and 5 am each day\n\n0 3 * * 0  at 3 am on Sunday\n\n0 3 5 * *  At 03:00 AM, on day 5 of the month\n\n0 3 5,19 * * At 03:00 AM, on day 5 and 19 of the month\n\n0 3 1/4 * * At 03:00 AM, every 4 days</code></pre>"},{"location":"developement/#indexing-approaches","title":"Indexing Approaches","text":"<p>The following approaches</p> <ul> <li>Divide up the sources by sitemap and sitegraph</li> <li>Also divide by production and queue sources</li> </ul> <p>The above will result in at most 4 initial sets.</p> <p>We can then use the docker approach</p> <pre><code>./gleanerDocker.sh -cfg /gleaner/wd/rundir/oih_queue.yaml  --source cioosatlantic</code></pre> <p>to run indexes on specific sources in these configuration files.  </p>"},{"location":"developement/#references","title":"References","text":"<ul> <li>Simple Dagster example</li> </ul>"},{"location":"monitoring_workflows/","title":"Monitoring workfows","text":""},{"location":"monitoring_workflows/#scheduler-interface","title":"scheduler interface","text":""},{"location":"monitoring_workflows/#check-the-run-interface","title":"check the run interface","text":"<p>http://localhost:3000/runs</p> <p>If there is a failure, click on the runid of the run, then you can look at the run log</p>"},{"location":"monitoring_workflows/#portainer-status","title":"portainer status","text":"<p>The gleaner and nabu are run as services are prefixed with sch_ pattern is</p> <p>sch_PROJECT_step</p> <p>so if it looks like something is not working, find the container starting with sch_project_step,</p> <p>then go into a terminal, </p> <p>you may need to use bin/sh</p> <pre><code>cd logs\nls -l \ntail some log name</code></pre>"},{"location":"quick/","title":"Notes","text":""},{"location":"quick/#implementation-network-builder","title":"Implementation network builder","text":"<p>The work for building the dagster containers for a given implementation network starts in  the directory <code>scheduler/dagster/implnets</code>.  At this time most of this can be driven by the Makefile.</p> <p>1) Make sure your gleanerconfig.yaml file is in the configs/NETWORK directory where    NETWORK is your implmentation network like eco, iow, etc. 2) Check the VERSION file and make sure it has a value you want in it to be tagged to the containers. 3) <code>make eco-clean</code>  will remove any existing generated code from the ./generatedCode/implnet-NETWORK directory 4) <code>make eco-generate</code> will build the code new.  Set the -d N in the makefile to a value N that is the number    of days you want the runs to cycle over.  So 30 would mean they run once every 30 days.  If you want some providers    to index at different rates you currently need to go in and edit the associated provider schedules file editing the    line <code>@schedule(cron_schedule=\"0 12 * * 6\", job=implnet_job_amgeo, execution_timezone=\"US/Central\")</code> with a     cron value you want. 5) <code>make eco-build</code> builds the Docker images following the build file ./build/Docker file.  Note this uses the     command line argument <code>--build-arg implnet=eco</code> to set the implementation NETWORK so that the correct build code     from generatedCode/NETWORK is copied over 6) <code>make eco-push</code> push to your container registry of choice, here docker.io</p>"},{"location":"quick/#compose-environment-and-docker-api-assets","title":"Compose, Environment and Docker API Assets","text":"<p>1) You will need the (or need to make) the portainer access token      from your https://portainer.geocodes-aws-dev.earthcube.org/#!/account 2) You will need a valid Gleaner configuration file named gleanerconfig.yaml and a nabu config named nabuconfig.yaml 3) You will need the schema.org context files places in a directory assets  get each of the http and https versions    1) <code>wget https://schema.org/version/latest/schemaorg-current-https.jsonld</code>    2) <code>wget https://schema.org/version/latest/schemaorg-current-http.jsonld</code> 4) Generate the archive files for Gleaner and Nabu.  Note the path to the context     files should map with what is in the configuration files    1) <code>tar -zcf ./archives/NabuCfg.tgz ./nabuconfig.yaml ./assets</code>    2) <code>tar -zcf ./archives/GleanerCfg.tgz ./gleanerconfig.yaml ./assets</code> 5) The archives .tgz files named NabuCfg.tgz and GleanerCfg.tgz need to be copied to the schedule prefix    in your bucket used for Gleaner    1) <code>mc cp GleanerCfg.tgz NabuCfg.tgz  gleaner/scheduler/configs</code>    2) Make sure GLEANERIO_NABU_ARCHIVE_OBJECT and GLEANERIO_GLEANER_ARCHIVE_OBJECT reflect this location in the .env file 6) Next you will need to build the scheduler containers for your implementation network. Push these containers    to your container registry of choice and make sure the values are set in the .env file and that    the containers are available to Portainer or will get pulled on use.   These are the image files in the     compose file and also the images notes in the environment variables GLEANERIO_GLEANER_IMAGE and GLEANERIO_NABU_IMAGE    in the .env file.</p> <p>At this point you are ready to move to your Docker or Portainer environment and deploy the  compose and environment files.  </p>"},{"location":"quick/#notes_1","title":"Notes","text":"<p>1) I do not have the API call to ensure/check/pull and image used by the API, so these images need to be     pulled down manually at this time.  These are the images noted by the .env files at     <code>GLEANERIO_GLEANER_IMAGE=fils/gleaner:v3.0.11-development-df</code> and     <code>GLEANERIO_NABU_IMAGE=fils/nabu:2.0.8-development</code></p>"},{"location":"assets/configs/","title":"Configs","text":""},{"location":"assets/configs/#about","title":"About","text":"<p>This directory holds some of the general configuration files that might be useful for running Gleaner and Nabu.  This section is broken down by the various communities (implementation networks).</p> <p>These are provided as examples.</p>"},{"location":"assets/configs/#tar-archive","title":"TAR archive","text":"<p>The tar archive must be compressed and must be named to align  with the archive ENV variable</p> <pre><code>GLEANERIO_GLEANER_ARCHIVE_OBJECT=scheduler/configs/GleanerCfg.tgz\nGLEANERIO_NABU_ARCHIVE_OBJECT=scheduler/configs/NabuCfg.tgz</code></pre> <pre><code> tar -zcf GleanerCfg.tgz ./gleanerconfig.yaml ./jsonldcontext.json\n ```\n\n```bash\n tar -zcf NabuCfg.tgz ./nabuconfig.yaml ./jsonldcontext.json ./assets</code></pre>"},{"location":"assets/configs/#environment-variables","title":"Environment variables","text":"<p>The command</p> <pre><code>source file.env</code></pre> <p>should set your values.  Be sure not to have spaces in  your vars.</p>"},{"location":"assets/configs/#docker-compose","title":"Docker Compose","text":"<p>Example compose file for use with Dagster is included here. It also is edited to read and express the shell variables  into the containers.</p>"},{"location":"images/appendix/","title":"Appendix","text":""},{"location":"images/appendix/#the-asset-file-flow-in-more-detail","title":"The asset file flow in more detail:","text":"<ul> <li>Creation of template files for the various operations, jobs and schedules</li> <li>Creation of the archive files that hold the configuration for the jobs run </li> <li>Environment file for the values needed by the operations</li> </ul>"}]}